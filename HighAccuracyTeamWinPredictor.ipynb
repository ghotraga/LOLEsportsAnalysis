{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023 = pd.read_csv('Data/2023.csv')\n",
    "df_2022 = pd.read_csv('Data/2022.csv')\n",
    "df_2021 = pd.read_csv('Data/2021.csv')\n",
    "df_2020 = pd.read_csv('Data/2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all dataframes into one\n",
    "df = pd.concat([df_2023, df_2022, df_2021, df_2020], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up data for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on data completeness and position\n",
    "df = df[(df['datacompleteness'] == 'complete') & (df['position'] == 'team')]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['datacompleteness', 'url', 'gameid', 'league', 'split', 'playoffs', 'date', 'game', 'participantid', 'position', 'playername', 'playerid', 'teamid', 'champion', 'firstbloodassist', 'firstbloodkill', 'firstbloodvictim', 'dragons (type unknown)', 'damageshare', 'earnedgoldshare', 'total cs', 'monsterkillsownjungle', 'monsterkillsenemyjungle', 'teamname', 'year', 'patch', 'teamkills', 'teamdeaths']\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Fill NaN values in specific columns\n",
    "df.fillna({'ban1': 'UNKNOWN', 'ban2': 'UNKNOWN', 'ban3': 'UNKNOWN', 'ban4': 'UNKNOWN', 'ban5': 'UNKNOWN', 'firstmidtower': 0}, inplace=True)\n",
    "\n",
    "champions = pd.concat([df['ban1'], df['ban2'], df['ban3'], df['ban4'], df['ban5']]).unique()\n",
    "\n",
    "# Convert categorical values to numerical values\n",
    "value_to_index = {value: index for index, value in enumerate(champions)}\n",
    "\n",
    "df['ban1'] = df['ban1'].map(value_to_index)\n",
    "df['ban2'] = df['ban2'].map(value_to_index)\n",
    "df['ban3'] = df['ban3'].map(value_to_index)\n",
    "df['ban4'] = df['ban4'].map(value_to_index)\n",
    "df['ban5'] = df['ban5'].map(value_to_index)\n",
    "\n",
    "df['side'] = df['side'].map({'Blue': 0, 'Red': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['result'])\n",
    "y = df['result']\n",
    "\n",
    "columns_to_exclude_scaling = ['side', 'ban1', 'ban2', 'ban3', 'ban4', 'ban5']\n",
    "columns_to_scale = df.columns.difference(columns_to_exclude_scaling)\n",
    "scaler = MinMaxScaler()\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model.fit(X, y)\n",
    "# Get feature importances\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Get indices of top 10 features\n",
    "top_indices = np.argsort(feature_importance)[-10:]\n",
    "\n",
    "# Get the corresponding feature names\n",
    "top_features = X.columns[top_indices]\n",
    "\n",
    "# Get the corresponding feature importances\n",
    "top_importance = feature_importance[top_indices]\n",
    "\n",
    "# Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_features, top_importance)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 10 Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 important features mapped to their column names\n",
    "top_10_features = np.argsort(feature_importance)[-10:]\n",
    "top_10_features = [X.columns[i] for i in top_10_features]\n",
    "top_10_features\n",
    "\n",
    "# Only keep these columns in the dataset\n",
    "X_train = X_train[top_10_features]\n",
    "X_test = X_test[top_10_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequential_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input_shape=(X_train.shape[1],), activation='relu', units=128),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_sequential_network()\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=1, batch_size=32, shuffle=True, use_multiprocessing=True, workers=4)\n",
    "\n",
    "# Plot the loss and accuracy curves for training and validation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(history.history['loss'], label='Train Loss')\n",
    "ax[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Loss')\n",
    "ax[1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
